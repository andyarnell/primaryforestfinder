{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6876640",
   "metadata": {},
   "source": [
    "Aim:  converting data from here (https://github.com/microsoft/RoadDetections/?tab=readme-ov-file) into shapefiles to upload to Google Earth Engine (GEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435689cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022a614",
   "metadata": {},
   "source": [
    "TSV to CSV \n",
    "- TSV: tab seperated file format - native format for microsoft roads data\n",
    "- CSV: comma separated file format. Chosen as GEE allows upload up to 10 GB isnteast of 2 GB for shapefiles\n",
    "\n",
    "Format differences: \n",
    "- TSV: has ISO3 in one column and what appears to be a GeoJSON data in another column \n",
    "- CSV: geometry stored as a column and properties in other columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def convert_geojson_column_to_csv(\n",
    "    input_path: str,\n",
    "    output_path: str = \"parsed_output.csv\",\n",
    "    input_format: str = \"tsv\",\n",
    "    max_rows: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a CSV/TSV with ISO3 and GeoJSON column to a flat CSV with geometry and properties.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Path to the input file (CSV or TSV).\n",
    "    - output_path (str): Path to save the parsed CSV.\n",
    "    - input_format (str): Either 'csv' or 'tsv'.\n",
    "    - max_rows (int or None): Max number of rows to process. Use None to process all rows.\n",
    "    \"\"\"\n",
    "    def parse_line(json_str):\n",
    "        json_str = json_str.replace('\"\"', '\"').strip('\"')\n",
    "        data = json.loads(json_str)\n",
    "        coordinates = data[\"geometry\"][\"coordinates\"]\n",
    "        geometry_type = data[\"geometry\"][\"type\"]\n",
    "        properties = data[\"properties\"]\n",
    "\n",
    "        if geometry_type == \"LineString\":\n",
    "            coords_wkt = \", \".join([f\"{x[0]} {x[1]}\" for x in coordinates])\n",
    "            geometry = f\"LINESTRING ({coords_wkt})\"\n",
    "        else:\n",
    "            geometry = \"UNKNOWN\"\n",
    "\n",
    "        return {\n",
    "            # \"iso3\": iso3,\n",
    "            \"geometry\": geometry,\n",
    "            **properties\n",
    "        }\n",
    "\n",
    "    # === Load input file ===\n",
    "    sep = '\\t' if input_format == \"tsv\" else ','\n",
    "    df_raw = pd.read_csv(input_path, sep=sep, header=None, names=[\"iso3\", \"geojson_str\"])\n",
    "\n",
    "    if max_rows is not None:\n",
    "        df_raw = df_raw.head(max_rows)\n",
    "\n",
    "    # === Parse all rows ===\n",
    "    parsed_rows = [parse_line(row.geojson_str) for _, row in df_raw.iterrows()]\n",
    "    df_parsed = pd.DataFrame(parsed_rows)\n",
    "\n",
    "    # === Save to CSV ===\n",
    "    df_parsed.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Parsed {len(df_parsed)} rows and saved to '{output_path}'.\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "def split_large_csv(input_file, output_prefix, num_chunks=2, chunk_size=None, rows_estimate=None):\n",
    "    \"\"\"\n",
    "    Split a large CSV file into multiple smaller files.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_file : str\n",
    "        Path to the input CSV file\n",
    "    output_prefix : str\n",
    "        Prefix for the output files, will be followed by _part1, _part2, etc.\n",
    "    num_chunks : int\n",
    "        Number of chunks to split the file into (default: 2)\n",
    "    chunk_size : int, optional\n",
    "        Size of each chunk in rows (overrides num_chunks if provided)\n",
    "    rows_estimate : int, optional\n",
    "        Estimated number of rows in the file (to avoid counting)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        List of paths to the created output files\n",
    "    \"\"\"\n",
    "    print(f\"Starting to split {input_file}\")\n",
    "    output_files = []\n",
    "    \n",
    "    # Get the file size for progress reporting\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    print(f\"File size: {file_size / (1024 * 1024 * 1024):.2f} GB\")\n",
    "    \n",
    "    # Determine total rows if not provided\n",
    "    if not rows_estimate:\n",
    "        print(\"Counting rows in the file (this may take a while)...\")\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            rows_estimate = sum(1 for _ in f) - 1  # Subtract 1 for header\n",
    "        print(f\"Found {rows_estimate} rows\")\n",
    "    else:\n",
    "        print(f\"Using estimated row count: {rows_estimate}\")\n",
    "    \n",
    "    # Calculate rows per chunk\n",
    "    if chunk_size:\n",
    "        rows_per_chunk = chunk_size\n",
    "        estimated_chunks = math.ceil(rows_estimate / rows_per_chunk)\n",
    "        print(f\"Will create approximately {estimated_chunks} chunks with {rows_per_chunk} rows each\")\n",
    "    else:\n",
    "        rows_per_chunk = math.ceil(rows_estimate / num_chunks)\n",
    "        print(f\"Will create {num_chunks} chunks with approximately {rows_per_chunk} rows each\")\n",
    "    \n",
    "    # Read and write in chunks\n",
    "    print(\"Starting to write chunks...\")\n",
    "    chunks_written = 0\n",
    "    current_chunk = 1\n",
    "    \n",
    "    # Use pandas to process in chunks\n",
    "    for chunk in pd.read_csv(input_file, chunksize=100000):  # Process 100k rows at a time\n",
    "        # Determine which output file this chunk belongs to\n",
    "        output_file = f\"{output_prefix}_part{current_chunk}.csv\"\n",
    "        \n",
    "        # If this is a new file, write with header\n",
    "        if output_file not in output_files:\n",
    "            chunk.to_csv(output_file, index=False, mode='w')\n",
    "            output_files.append(output_file)\n",
    "            print(f\"Created new file: {output_file}\")\n",
    "        else:\n",
    "            # Append to existing file without header\n",
    "            chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "        \n",
    "        # Update counters\n",
    "        chunks_written += len(chunk)\n",
    "        \n",
    "        # Progress reporting\n",
    "        if chunks_written % 1000000 == 0:\n",
    "            print(f\"Processed {chunks_written:,} rows ({chunks_written/rows_estimate*100:.1f}%)\")\n",
    "        \n",
    "        # Check if we need to move to the next file\n",
    "        if chunks_written >= current_chunk * rows_per_chunk and current_chunk < num_chunks:\n",
    "            current_chunk += 1\n",
    "    \n",
    "    print(f\"Successfully split {input_file} into {len(output_files)} files\")\n",
    "    print(f\"Output files: {output_files}\")\n",
    "    return output_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\raw\\roads\\microsoft\"\n",
    "\n",
    "output_folder = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23450ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using os.path.join for proper path construction\n",
    "import os\n",
    "input_file_path = os.path.join(in_folder, \"Caribbean\", \"Caribbean.tsv\")\n",
    "output_path_csv = os.path.join(output_folder, \"Caribbean.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b76bf",
   "metadata": {},
   "source": [
    "List folders in the Microsoft roads data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "# List all items in the directory \n",
    "country_folders = os.listdir(in_folder)\n",
    "\n",
    "# using fnmatch for more complex wildcard patterns\n",
    "\n",
    "# remove those with .zip extension\n",
    "country_folders = [f for f in country_folders if not fnmatch.fnmatch(f, '*.zip')]\n",
    "\n",
    "#if you want to remove specific folders, you can use patterns like:\n",
    "country_folders = [f for f in country_folders if not fnmatch.fnmatch(f, '*Northern_America*')]\n",
    "# country_folders = [f for f in country_folders if not fnmatch.fnmatch(f, '*Eastern_Africa*')]\n",
    "\n",
    "country_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209085b7",
   "metadata": {},
   "source": [
    "Get list of countries that need processing (have TSV but no CSV yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af66e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countries_to_process = []\n",
    "for each in country_folders:\n",
    "    input_file_path = os.path.join(in_folder, each, f\"{each}.tsv\")\n",
    "    output_path_csv = os.path.join(output_folder, f\"{each}.csv\")\n",
    "    \n",
    "    # Check if input exists but output doesn't\n",
    "    if os.path.exists(input_file_path) and not os.path.exists(output_path_csv):\n",
    "        countries_to_process.append(each)\n",
    "        print(f\"Will process: {each}\")\n",
    "    elif not os.path.exists(input_file_path):\n",
    "        print(f\"Input file not found: {input_file_path}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"Output already exists for {each}. Skipping.\")\n",
    "\n",
    "print(f\"Found {len(countries_to_process)} countries to process out of {len(country_folders)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc3249",
   "metadata": {},
   "source": [
    "Process only the countries that need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoJSON column to CSV\n",
    "# Loop through the countries to process\n",
    "for each in countries_to_process:\n",
    "    # Construct the paths (no need for checking again)\n",
    "    input_file_path = os.path.join(in_folder, each, f\"{each}.tsv\")\n",
    "    output_path_csv = os.path.join(output_folder, f\"{each}.csv\")\n",
    "    \n",
    "    # Convert the TSV to CSV\n",
    "    try:\n",
    "        print(f\"Converting {each}...\")\n",
    "        convert_geojson_column_to_csv(input_file_path, output_path_csv)\n",
    "        print(f\"Conversion complete for {each}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {each}: {str(e)}\")\n",
    "\n",
    "print(\"All processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a9e66",
   "metadata": {},
   "source": [
    "Split large files i.e., CSVs over 10GB (only needed to split N America into two parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433ad4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to split C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe.csv\n",
      "File size: 3.51 GB\n",
      "Counting rows in the file (this may take a while)...\n",
      "Found 22992884 rows\n",
      "Will create 3 chunks with approximately 7664295 rows each\n",
      "Starting to write chunks...\n",
      "Created new file: C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe_part1.csv\n",
      "Processed 1,000,000 rows (4.3%)\n",
      "Processed 2,000,000 rows (8.7%)\n",
      "Processed 3,000,000 rows (13.0%)\n",
      "Processed 4,000,000 rows (17.4%)\n",
      "Processed 5,000,000 rows (21.7%)\n",
      "Processed 6,000,000 rows (26.1%)\n",
      "Processed 7,000,000 rows (30.4%)\n",
      "Created new file: C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe_part2.csv\n",
      "Processed 8,000,000 rows (34.8%)\n",
      "Processed 9,000,000 rows (39.1%)\n",
      "Processed 10,000,000 rows (43.5%)\n",
      "Processed 11,000,000 rows (47.8%)\n",
      "Processed 12,000,000 rows (52.2%)\n",
      "Processed 13,000,000 rows (56.5%)\n",
      "Processed 14,000,000 rows (60.9%)\n",
      "Processed 15,000,000 rows (65.2%)\n",
      "Created new file: C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe_part3.csv\n",
      "Processed 16,000,000 rows (69.6%)\n",
      "Processed 17,000,000 rows (73.9%)\n",
      "Processed 18,000,000 rows (78.3%)\n",
      "Processed 19,000,000 rows (82.6%)\n",
      "Processed 20,000,000 rows (87.0%)\n",
      "Processed 21,000,000 rows (91.3%)\n",
      "Processed 22,000,000 rows (95.7%)\n",
      "Successfully split C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe.csv into 3 files\n",
      "Output files: ['C:\\\\Users\\\\Arnell\\\\OneDrive - Food and Agriculture Organization\\\\project_work\\\\p0002_primary_forest_support\\\\work_in_progress\\\\roads\\\\microsoft\\\\processed_csvs_w_geom\\\\Eastern_Europe_part1.csv', 'C:\\\\Users\\\\Arnell\\\\OneDrive - Food and Agriculture Organization\\\\project_work\\\\p0002_primary_forest_support\\\\work_in_progress\\\\roads\\\\microsoft\\\\processed_csvs_w_geom\\\\Eastern_Europe_part2.csv', 'C:\\\\Users\\\\Arnell\\\\OneDrive - Food and Agriculture Organization\\\\project_work\\\\p0002_primary_forest_support\\\\work_in_progress\\\\roads\\\\microsoft\\\\processed_csvs_w_geom\\\\Eastern_Europe_part3.csv']\n"
     ]
    }
   ],
   "source": [
    "# input_csv = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Northern_America.csv\"\n",
    "# output_prefix = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Northern_America_split\"\n",
    "\n",
    "input_csv = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe.csv\"\n",
    "output_prefix = r\"C:\\Users\\Arnell\\OneDrive - Food and Agriculture Organization\\project_work\\p0002_primary_forest_support\\work_in_progress\\roads\\microsoft\\processed_csvs_w_geom\\Eastern_Europe\"\n",
    "\n",
    "# Split into 2 chunks using the estimated row count of 70 million\n",
    "output_files = split_large_csv(\n",
    "    input_file=input_csv,\n",
    "    output_prefix=output_prefix,\n",
    "    num_chunks=3,\n",
    "    # rows_estimate=15000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3459b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
